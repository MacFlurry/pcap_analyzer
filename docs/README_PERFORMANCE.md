# Documentation Performance - PCAP Analyzer Web

**Agent:** Performance
**Date:** 2025-12-12
**Statut:** Pr√™t pour impl√©mentation

---

## Vue d'Ensemble

Cette documentation couvre l'ensemble des optimisations de performance pour garantir que l'overhead web reste **<10% vs baseline CLI** (55s pour 131k paquets).

**Principe cl√©:** Les optimisations CPU/m√©moire sont D√âJ√Ä en place dans le code CLI. L'objectif est de **pr√©server** ces performances dans le conteneur web, pas de les r√©optimiser.

---

## Documents Disponibles

### üìã [PLAN_PERFORMANCE.md](./PLAN_PERFORMANCE.md) - Plan Complet

**Usage:** Document de r√©f√©rence principal (200+ KB)

**Contenu:**
- Baseline CLI et objectifs web d√©taill√©s
- Optimisations par domaine (7 sections)
- Scripts de benchmarking complets
- Monitoring et m√©triques
- Checklist de validation

**Sections cl√©s:**
1. Baseline et Objectifs
2. Image Docker (<250 MB)
3. Runtime Performance (FastAPI async)
4. Optimisation M√©moire (d√©j√† optimis√©)
5. Optimisation CPU (hybrid mode d√©j√† optimis√©)
6. Optimisation Stockage (cleanup agressif)
7. Optimisation R√©seau (gzip, SSE)
8. Analyses Concurrentes (queue, timeout)
9. Scripts de Benchmarking
10. Monitoring et M√©triques
11. Checklist de Validation

**Quand le lire:**
- Avant de commencer l'impl√©mentation web
- Pour comprendre les d√©tails techniques
- Pour r√©f√©rence lors du d√©veloppement

---

### üèóÔ∏è [ARCHITECTURE_PERFORMANCE.md](./ARCHITECTURE_PERFORMANCE.md) - Vue Architecture

**Usage:** Compr√©hension visuelle du syst√®me

**Contenu:**
- Diagrammes ASCII du flow complet
- Architecture conteneur Docker
- Flow de donn√©es d√©taill√© (4 sc√©narios)
- Optimisations cl√©s expliqu√©es
- Anti-patterns √† √©viter
- Best practices

**Sc√©narios couverts:**
1. Upload PCAP (chunked transfer)
2. Analyse background (hybrid mode)
3. Progress monitoring (SSE)
4. Report download (streaming)

**Quand le lire:**
- Pour comprendre l'architecture globale
- Avant de coder les endpoints FastAPI
- Pour visualiser les flows de donn√©es

---

### ‚úÖ [CHECKLIST_PERFORMANCE.md](./CHECKLIST_PERFORMANCE.md) - Validation

**Usage:** Document de validation avant livraison

**Contenu:**
- 7 cat√©gories de tests
- Commandes exactes √† ex√©cuter
- Crit√®res PASS/FAIL clairs
- Troubleshooting si √©chec
- R√©sum√© validation finale

**Cat√©gories:**
1. Performance (CLI vs Web, Memory, CPU, Load)
2. Image Docker (taille, build time)
3. Runtime (upload, SSE, concurrence)
4. S√©curit√© (non-root, security opts, validation)
5. Observabilit√© (health, logs, monitoring)
6. Scalabilit√© (horizontal scaling, volumes)
7. Cleanup (PCAP, rapports, logs)

**Quand l'utiliser:**
- Avant chaque livraison (sprint)
- Pour validation finale (production)
- Pour debugging si r√©gressions

---

### üîß [scripts/README.md](../scripts/README.md) - Scripts Benchmarking

**Usage:** Documentation scripts de test

**Contenu:**
- 4 scripts de benchmarking d√©taill√©s
- Usage et exemples pour chaque script
- Output attendu
- Crit√®res de validation
- Workflow complet de validation
- Troubleshooting par script

**Scripts:**
1. `benchmark_cli_vs_web.py` - Overhead <10%
2. `benchmark_memory.py` - Memory profiling
3. `profile_analysis.py` - CPU bottlenecks
4. `locustfile.py` - Load testing

**Quand l'utiliser:**
- Pour ex√©cuter les benchmarks
- Pour comprendre les outputs
- Pour debugging performance

---

### üìä [DECISIONS_TECHNIQUES.md](./DECISIONS_TECHNIQUES.md) - D√©cisions Stack

**Usage:** R√©f√©rence d√©cisions architecture web

**Contenu:**
- Stack technique valid√©e
- Comparatifs avec sources
- Architecture globale
- Flow de donn√©es
- D√©pendances
- S√©curit√©
- Performance attendue
- Plan impl√©mentation

**D√©cisions cl√©s:**
- Backend: FastAPI + Uvicorn
- Frontend: Vanilla JS + Tailwind
- Communication: SSE (pas WebSockets)
- Stockage: Filesystem + SQLite
- Queue: asyncio.Queue (in-process)
- Image: python:3.11-slim-bookworm
- Multi-stage: OUI (3 stages)
- Cleanup: APScheduler (in-process)

**Quand le lire:**
- Avant impl√©mentation (obligatoire)
- Pour comprendre les choix techniques
- Pour √©viter de r√©inventer la roue

---

## Quick Start - Validation Performance

### 1. Mesurer Baseline CLI (Obligatoire)

```bash
# Installer hyperfine (recommand√©)
brew install hyperfine  # macOS
# ou apt install hyperfine  # Linux

# Benchmark CLI (10 runs)
hyperfine --warmup 1 --runs 10 \
    'python -m src.cli analyze tests/data/sample.pcap --no-report'

# Output attendu: ~55s ¬± 2s
```

### 2. D√©ployer Application Web

```bash
# Build image
docker build -t pcap-analyzer:latest .

# V√©rifier taille
docker images pcap-analyzer:latest
# Target: <250 MB

# D√©marrer
docker-compose up -d

# V√©rifier health
curl http://localhost:8000/health
```

### 3. Valider Performance Web

```bash
# 1. Overhead CLI vs Web
python scripts/benchmark_cli_vs_web.py tests/data/sample.pcap
# ‚úÖ Target: Overhead <10%

# 2. Memory profiling
python scripts/benchmark_memory.py tests/data/sample.pcap
# ‚úÖ Target: Peak <600MB pour 26MB PCAP

# 3. Load test (5 min)
pip install locust
locust -f scripts/locustfile.py \
    --host=http://localhost:8000 \
    --users 10 \
    --spawn-rate 2 \
    --run-time 5m \
    --headless
# ‚úÖ Target: Failure rate <10%
```

### 4. Checklist Finale

Suivre **CHECKLIST_PERFORMANCE.md** pour validation compl√®te:
- [ ] Performance (4 tests)
- [ ] Image Docker (2 tests)
- [ ] Runtime (3 tests)
- [ ] S√©curit√© (3 tests)
- [ ] Observabilit√© (3 tests)
- [ ] Cleanup (3 tests)

---

## M√©triques Cibles - R√©sum√©

| M√©trique | CLI Baseline | Web Target | Overhead Max | Script |
|----------|--------------|------------|--------------|--------|
| **Temps total** | 55s | <60s | +9% | benchmark_cli_vs_web.py |
| **Throughput** | 2,382 pkt/s | >2,183 pkt/s | -8% | benchmark_cli_vs_web.py |
| **Peak memory** | ~500 MB | <600 MB | +20% | benchmark_memory.py |
| **Upload 100MB** | N/A | <5s | N/A | Manual |
| **SSE latency** | N/A | <500ms | N/A | Manual |
| **Image size** | N/A | <250 MB | N/A | docker images |
| **Failure rate** | 0% | <10% | N/A | locustfile.py |

---

## Workflow D√©veloppement

### Phase 1: Lecture Documentation (1h)

1. Lire **DECISIONS_TECHNIQUES.md** (d√©cisions stack)
2. Lire **ARCHITECTURE_PERFORMANCE.md** (vue architecture)
3. Parcourir **PLAN_PERFORMANCE.md** (sections pertinentes)

### Phase 2: Impl√©mentation Backend (Sprint 1-2)

1. Cr√©er structure FastAPI (`web/main.py`)
2. Impl√©menter endpoints:
   - POST /upload (chunked, validation)
   - GET /progress/:id (SSE)
   - GET /status/:id (SQLite)
   - GET /report/:id (streaming)
   - GET /health (metrics)
3. Wrapper `analyze_pcap_hybrid()` avec SSE callbacks
4. Setup queue asyncio.Queue (maxsize=5)
5. Background worker (1 seul, CPU-bound)
6. APScheduler cleanup (hourly)

### Phase 3: Tests Performance (Sprint 3)

1. Benchmark CLI vs Web
   ```bash
   python scripts/benchmark_cli_vs_web.py tests/data/sample.pcap
   ```
2. Memory profiling
   ```bash
   python scripts/benchmark_memory.py tests/data/sample.pcap --detailed
   ```
3. CPU profiling (si bottlenecks d√©tect√©s)
   ```bash
   python scripts/profile_analysis.py tests/data/sample.pcap --output profile.prof
   snakeviz profile.prof
   ```

### Phase 4: Load Testing (Sprint 4)

1. Test nominal (10 users, 5 min)
   ```bash
   locust -f scripts/locustfile.py --host=http://localhost:8000 \
       --users 10 --spawn-rate 2 --run-time 5m --headless
   ```
2. Stress test (queue saturation)
   ```bash
   locust -f scripts/locustfile.py --host=http://localhost:8000 \
       --users 20 --spawn-rate 5 --run-time 10m --headless
   ```

### Phase 5: Validation Finale (Sprint 5)

1. Suivre **CHECKLIST_PERFORMANCE.md** int√©gralement
2. Corriger √©checs (voir troubleshooting)
3. Valider tous crit√®res PASS
4. Livraison production

---

## Points Cl√©s - √Ä Retenir

### ‚úÖ DO

1. **R√©utiliser code CLI existant**
   - `analyze_pcap_hybrid()` D√âJ√Ä optimis√© (hybrid dpkt+Scapy)
   - Juste wrapper avec SSE callbacks
   - Pas de duplication code

2. **Utiliser optimisations existantes**
   - `StreamingProcessor` pour fichiers >100MB
   - `MemoryOptimizer` avec GC cooldown
   - Pas besoin de r√©optimiser!

3. **Async pour I/O, background pour CPU**
   - FastAPI async pour upload/download
   - Background worker pour analyse (CPU-bound)
   - 1 seul worker uvicorn (pas de gain multi-worker)

4. **Mesurer avant d'optimiser**
   - Baseline CLI: 55s (r√©f√©rence absolue)
   - Benchmark apr√®s chaque modif
   - Optimisation pr√©coce = racine du mal

### ‚ùå DON'T

1. **Ne PAS r√©optimiser l'analyseur**
   - Code CLI d√©j√† optimis√© (hybrid mode)
   - Focus sur pr√©servation performance

2. **Ne PAS utiliser multiple workers**
   - Analyse = CPU-bound (100% d'un core)
   - Multiple workers = contention CPU

3. **Ne PAS load PCAP en m√©moire**
   - Toujours chunked transfer (1MB chunks)
   - StreamingProcessor pour analyse

4. **Ne PAS spam GC**
   - Cooldown 5s entre GC (d√©j√† impl√©ment√©)
   - Skip si 3 GC vides cons√©cutifs

---

## Troubleshooting Rapide

### Overhead >10%

**Diagnostic:**
```bash
# Profiler CPU
python scripts/profile_analysis.py tests/data/sample.pcap
```

**Solutions:**
- V√©rifier limites Docker CPU/Memory
- V√©rifier pas d'autres processus
- Optimiser hot paths (si bottlenecks >20%)

---

### Memory >4GB

**Diagnostic:**
```bash
# Memory profiling d√©taill√©
python scripts/benchmark_memory.py tests/data/sample.pcap --detailed
```

**Solutions:**
- V√©rifier streaming mode activ√©
- V√©rifier GC triggering
- Chercher memory leaks (top allocations)

---

### Image >250MB

**Diagnostic:**
```bash
docker history pcap-analyzer:latest
```

**Solutions:**
- V√©rifier .dockerignore (tests, docs exclus)
- V√©rifier pip --no-cache-dir
- V√©rifier apt clean dans Dockerfile

---

### Load Test Fail

**Diagnostic:**
```bash
# Logs pendant load test
docker logs -f pcap-analyzer
docker stats pcap-analyzer
```

**Solutions:**
- V√©rifier queue handling (503 si full)
- V√©rifier timeout 30min
- Augmenter limites Docker

---

## Support

**Questions techniques:**
- Consulter **PLAN_PERFORMANCE.md** (d√©tails)
- Consulter **ARCHITECTURE_PERFORMANCE.md** (visuel)

**Validation:**
- Suivre **CHECKLIST_PERFORMANCE.md**
- Ex√©cuter scripts dans **scripts/README.md**

**D√©cisions architecture:**
- R√©f√©rence **DECISIONS_TECHNIQUES.md**

---

## Changements Futurs (Post-MVP)

### Si Scaling Requis (>10 req/sec)

1. Migrer vers **Celery + Redis**
   - Queue distribu√©e (partage entre containers)
   - Workers d√©di√©s (scalables)
   - R√©sultats persist√©s

2. **Prometheus + Grafana**
   - M√©triques d√©taill√©es
   - Dashboards temps r√©el
   - Alertes automatiques

3. **Kubernetes HPA**
   - Auto-scaling horizontal
   - Load balancing natif
   - Health checks avanc√©s

**Note:** Pas requis pour MVP (queue in-process suffit)

---

**Prochaine √©tape:** Impl√©mentation backend FastAPI (Agent D√©veloppeur)
**R√©f√©rence:** DECISIONS_TECHNIQUES.md + ARCHITECTURE_PERFORMANCE.md
