#!/usr/bin/env python3
"""
Audit Log Analysis Tool

This script analyzes security audit logs generated by the NIST-compliant audit logger.
It provides summaries, filters, and exports for security monitoring and compliance reporting.

Features:
- Parse structured JSON audit logs
- Filter by date range, event type, severity
- Generate summary reports
- Export to CSV for SIEM ingestion
- Identify security incidents
- Compliance reporting

Usage:
    # View all events
    python scripts/analyze_audit_log.py

    # Filter by date range
    python scripts/analyze_audit_log.py --start 2025-12-01 --end 2025-12-31

    # Filter by event type
    python scripts/analyze_audit_log.py --event-type "security.*"

    # Filter by severity
    python scripts/analyze_audit_log.py --severity CRITICAL,ALERT

    # Export to CSV
    python scripts/analyze_audit_log.py --export-csv audit_report.csv

    # Security incidents only
    python scripts/analyze_audit_log.py --security-only

    # Summary report
    python scripts/analyze_audit_log.py --summary

Author: PCAP Analyzer Security Team
Date: 2025-12-20
"""

import argparse
import csv
import json
import re
import sys
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

# Default audit log location
DEFAULT_AUDIT_LOG = "logs/audit/security_audit.log"


class AuditLogAnalyzer:
    """
    Analyzer for NIST-compliant audit logs.

    Parses structured JSON audit records and provides analysis,
    filtering, and reporting capabilities.
    """

    def __init__(self, log_file: str):
        """
        Initialize analyzer.

        Args:
            log_file: Path to audit log file
        """
        self.log_file = Path(log_file)
        self.records: List[Dict[str, Any]] = []

    def load_records(self) -> int:
        """
        Load audit records from log file.

        Returns:
            Number of records loaded
        """
        if not self.log_file.exists():
            print(f"Error: Audit log not found: {self.log_file}", file=sys.stderr)
            return 0

        with open(self.log_file, "r") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    record = json.loads(line.strip())
                    self.records.append(record)
                except json.JSONDecodeError as e:
                    print(f"Warning: Invalid JSON at line {line_num}: {e}", file=sys.stderr)

        return len(self.records)

    def filter_by_date_range(
        self, start_date: Optional[str] = None, end_date: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Filter records by date range.

        Args:
            start_date: Start date (ISO 8601 format)
            end_date: End date (ISO 8601 format)

        Returns:
            Filtered records
        """
        filtered = self.records

        if start_date:
            start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            filtered = [
                r for r in filtered
                if datetime.fromisoformat(r['timestamp'].replace('Z', '+00:00')) >= start_dt
            ]

        if end_date:
            end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
            filtered = [
                r for r in filtered
                if datetime.fromisoformat(r['timestamp'].replace('Z', '+00:00')) <= end_dt
            ]

        return filtered

    def filter_by_event_type(self, pattern: str) -> List[Dict[str, Any]]:
        """
        Filter records by event type pattern (regex).

        Args:
            pattern: Event type pattern (e.g., "security.*", "file.validation.*")

        Returns:
            Filtered records
        """
        regex = re.compile(pattern)
        return [r for r in self.records if regex.search(r['event_type'])]

    def filter_by_severity(self, severities: List[str]) -> List[Dict[str, Any]]:
        """
        Filter records by severity levels.

        Args:
            severities: List of severity levels (e.g., ["CRITICAL", "ALERT"])

        Returns:
            Filtered records
        """
        severities_upper = [s.upper() for s in severities]
        return [r for r in self.records if r['severity'] in severities_upper]

    def filter_by_outcome(self, outcomes: List[str]) -> List[Dict[str, Any]]:
        """
        Filter records by outcome.

        Args:
            outcomes: List of outcomes (e.g., ["FAILURE", "BLOCKED"])

        Returns:
            Filtered records
        """
        outcomes_upper = [o.upper() for o in outcomes]
        return [r for r in self.records if r['outcome'] in outcomes_upper]

    def get_security_events(self) -> List[Dict[str, Any]]:
        """
        Get only security-related events.

        Returns:
            Security event records
        """
        security_prefixes = ["security.", "auth."]
        return [
            r for r in self.records
            if any(r['event_type'].startswith(prefix) for prefix in security_prefixes)
        ]

    def generate_summary(self) -> Dict[str, Any]:
        """
        Generate summary statistics for audit log.

        Returns:
            Dictionary with summary statistics
        """
        if not self.records:
            return {"error": "No records loaded"}

        # Basic counts
        total_records = len(self.records)
        event_types = Counter(r['event_type'] for r in self.records)
        severities = Counter(r['severity'] for r in self.records)
        outcomes = Counter(r['outcome'] for r in self.records)
        components = Counter(r.get('component', 'unknown') for r in self.records)

        # Time range
        timestamps = sorted([datetime.fromisoformat(r['timestamp'].replace('Z', '+00:00')) for r in self.records])
        time_range = {
            "earliest": timestamps[0].isoformat() if timestamps else None,
            "latest": timestamps[-1].isoformat() if timestamps else None,
        }

        # Security event stats
        security_events = self.get_security_events()
        security_count = len(security_events)
        security_types = Counter(r['event_type'] for r in security_events)

        # Failure analysis
        failures = [r for r in self.records if r['outcome'] in ['FAILURE', 'BLOCKED']]
        failure_count = len(failures)
        failure_types = Counter(r['event_type'] for r in failures)

        # User activity
        users = Counter(r.get('user') for r in self.records if r.get('user'))

        return {
            "total_records": total_records,
            "time_range": time_range,
            "event_types": dict(event_types.most_common(10)),
            "severities": dict(severities),
            "outcomes": dict(outcomes),
            "components": dict(components.most_common(10)),
            "security_events": {
                "total": security_count,
                "percentage": round(security_count / total_records * 100, 2),
                "types": dict(security_types.most_common(5)),
            },
            "failures": {
                "total": failure_count,
                "percentage": round(failure_count / total_records * 100, 2),
                "types": dict(failure_types.most_common(5)),
            },
            "users": dict(users.most_common(10)) if users else {},
        }

    def detect_incidents(self) -> List[Dict[str, Any]]:
        """
        Detect potential security incidents from audit log.

        Returns:
            List of detected incidents with details
        """
        incidents = []

        # Incident 1: Multiple authentication failures
        auth_failures = [r for r in self.records if r['event_type'] == 'auth.failure']
        if len(auth_failures) >= 3:
            # Group by user and host
            by_user_host = defaultdict(list)
            for r in auth_failures:
                key = (r.get('user', 'unknown'), r['details'].get('host', 'unknown'))
                by_user_host[key].append(r)

            for (user, host), failures in by_user_host.items():
                if len(failures) >= 3:
                    incidents.append({
                        "type": "BRUTE_FORCE_ATTEMPT",
                        "severity": "HIGH",
                        "description": f"Multiple authentication failures for {user}@{host}",
                        "count": len(failures),
                        "user": user,
                        "host": host,
                        "timestamps": [f['timestamp'] for f in failures[:5]],
                    })

        # Incident 2: Decompression bomb detection
        decompression_bombs = [r for r in self.records if r['event_type'] == 'security.decompression_bomb.detected']
        for bomb in decompression_bombs:
            incidents.append({
                "type": "DECOMPRESSION_BOMB",
                "severity": "CRITICAL",
                "description": "Decompression bomb detected",
                "file": bomb.get('file_path', 'unknown'),
                "expansion_ratio": bomb['details'].get('expansion_ratio', 'unknown'),
                "timestamp": bomb['timestamp'],
            })

        # Incident 3: Path traversal attempts
        path_traversal = [r for r in self.records if r['event_type'] == 'security.path_traversal.attempt']
        if path_traversal:
            incidents.append({
                "type": "PATH_TRAVERSAL_ATTACK",
                "severity": "CRITICAL",
                "description": "Path traversal attempts detected",
                "count": len(path_traversal),
                "timestamps": [r['timestamp'] for r in path_traversal[:5]],
            })

        # Incident 4: Resource limit violations
        resource_violations = [r for r in self.records if r['event_type'] == 'security.resource_limit.exceeded']
        if len(resource_violations) >= 3:
            incidents.append({
                "type": "RESOURCE_EXHAUSTION",
                "severity": "HIGH",
                "description": "Multiple resource limit violations",
                "count": len(resource_violations),
                "limit_types": list(set(r['details'].get('limit_type', 'unknown') for r in resource_violations)),
                "timestamps": [r['timestamp'] for r in resource_violations[:5]],
            })

        # Incident 5: Rate limit violations
        rate_limits = [r for r in self.records if r['event_type'] == 'auth.rate_limit']
        if rate_limits:
            incidents.append({
                "type": "RATE_LIMIT_VIOLATION",
                "severity": "MEDIUM",
                "description": "Rate limit violations detected",
                "count": len(rate_limits),
                "timestamps": [r['timestamp'] for r in rate_limits[:5]],
            })

        return incidents

    def export_to_csv(self, output_file: str, records: Optional[List[Dict[str, Any]]] = None) -> None:
        """
        Export audit records to CSV format.

        Args:
            output_file: Output CSV file path
            records: Records to export (default: all records)
        """
        records = records or self.records

        if not records:
            print("No records to export", file=sys.stderr)
            return

        # Flatten nested details field
        flattened = []
        for record in records:
            flat_record = {
                "timestamp": record.get("timestamp"),
                "event_type": record.get("event_type"),
                "severity": record.get("severity"),
                "outcome": record.get("outcome"),
                "component": record.get("component"),
                "user": record.get("user"),
                "hostname": record.get("hostname"),
                "process_id": record.get("process_id"),
                "file_path": record.get("file_path"),
                "source_ip": record.get("source_ip"),
                "session_id": record.get("session_id"),
                "record_id": record.get("record_id"),
            }

            # Add details as JSON string
            flat_record["details"] = json.dumps(record.get("details", {}))

            flattened.append(flat_record)

        # Write CSV
        with open(output_file, "w", newline="") as csvfile:
            fieldnames = flattened[0].keys()
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(flattened)

        print(f"Exported {len(flattened)} records to {output_file}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Analyze NIST-compliant security audit logs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # View summary
  python scripts/analyze_audit_log.py --summary

  # Filter by date range
  python scripts/analyze_audit_log.py --start 2025-12-01 --end 2025-12-31

  # Security events only
  python scripts/analyze_audit_log.py --security-only

  # Export to CSV
  python scripts/analyze_audit_log.py --export-csv audit_report.csv

  # Detect incidents
  python scripts/analyze_audit_log.py --incidents
        """,
    )

    parser.add_argument(
        "--log-file",
        default=DEFAULT_AUDIT_LOG,
        help=f"Path to audit log file (default: {DEFAULT_AUDIT_LOG})",
    )
    parser.add_argument("--start", help="Start date (ISO 8601 format)")
    parser.add_argument("--end", help="End date (ISO 8601 format)")
    parser.add_argument("--event-type", help="Event type pattern (regex)")
    parser.add_argument("--severity", help="Severity levels (comma-separated)")
    parser.add_argument("--outcome", help="Outcomes (comma-separated)")
    parser.add_argument("--security-only", action="store_true", help="Show security events only")
    parser.add_argument("--summary", action="store_true", help="Show summary statistics")
    parser.add_argument("--incidents", action="store_true", help="Detect security incidents")
    parser.add_argument("--export-csv", help="Export to CSV file")
    parser.add_argument("--limit", type=int, help="Limit number of records displayed")

    args = parser.parse_args()

    # Initialize analyzer
    analyzer = AuditLogAnalyzer(args.log_file)

    # Load records
    count = analyzer.load_records()
    if count == 0:
        print(f"No audit records found in {args.log_file}")
        sys.exit(1)

    print(f"Loaded {count} audit records from {args.log_file}\n")

    # Apply filters
    filtered_records = analyzer.records

    if args.start or args.end:
        filtered_records = analyzer.filter_by_date_range(args.start, args.end)
        print(f"After date filter: {len(filtered_records)} records\n")

    if args.event_type:
        analyzer.records = filtered_records
        filtered_records = analyzer.filter_by_event_type(args.event_type)
        print(f"After event type filter: {len(filtered_records)} records\n")

    if args.severity:
        analyzer.records = filtered_records
        severities = [s.strip() for s in args.severity.split(",")]
        filtered_records = analyzer.filter_by_severity(severities)
        print(f"After severity filter: {len(filtered_records)} records\n")

    if args.outcome:
        analyzer.records = filtered_records
        outcomes = [o.strip() for o in args.outcome.split(",")]
        filtered_records = analyzer.filter_by_outcome(outcomes)
        print(f"After outcome filter: {len(filtered_records)} records\n")

    if args.security_only:
        analyzer.records = filtered_records
        filtered_records = analyzer.get_security_events()
        print(f"Security events only: {len(filtered_records)} records\n")

    # Generate summary
    if args.summary:
        analyzer.records = filtered_records
        summary = analyzer.generate_summary()
        print("=" * 60)
        print("AUDIT LOG SUMMARY")
        print("=" * 60)
        print(json.dumps(summary, indent=2))
        print()

    # Detect incidents
    if args.incidents:
        analyzer.records = filtered_records
        incidents = analyzer.detect_incidents()
        print("=" * 60)
        print(f"SECURITY INCIDENTS DETECTED: {len(incidents)}")
        print("=" * 60)
        for i, incident in enumerate(incidents, 1):
            print(f"\nIncident #{i}:")
            print(json.dumps(incident, indent=2))
        print()

    # Export to CSV
    if args.export_csv:
        analyzer.export_to_csv(args.export_csv, filtered_records)
        print()

    # Display records
    if not (args.summary or args.incidents or args.export_csv):
        limit = args.limit or 20
        print("=" * 60)
        print(f"AUDIT RECORDS (showing {min(limit, len(filtered_records))} of {len(filtered_records)})")
        print("=" * 60)
        for record in filtered_records[:limit]:
            print(json.dumps(record, indent=2))
            print("-" * 60)


if __name__ == "__main__":
    main()
